# -*- coding: utf-8 -*-
"""RunPerception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mhv4n2rTiKuq9jMQ2_jRXFY6L21lXp25
"""

import cv2
import torch

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import math

import torch.nn as nn

import os

class CNN_layer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size_conv, kernel_size_pool, stride, padding, padding_pool):
        super(CNN_layer, self).__init__()

        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size_conv, stride=stride, padding=padding)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=kernel_size_pool, padding=padding_pool)
    def forward(self, x):
        out = (self.pool(self.relu(self.conv(x))))
        return out

class BasicCNN(nn.Module):
    def __init__(self, output_size = 14):
        super(BasicCNN, self).__init__()

        self.conv1 = CNN_layer(in_channels=3, out_channels=32, kernel_size_conv=(3,3), kernel_size_pool=(2,2), stride=(1,1), padding=(0,0), padding_pool=(0,0))
        self.conv2 = CNN_layer(in_channels=32, out_channels=64, kernel_size_conv=(3,3), kernel_size_pool=(2,2), stride=(1,1), padding=(0,0), padding_pool=(0,0))
        self.conv3 = CNN_layer(in_channels=64, out_channels=128, kernel_size_conv=(3,3), kernel_size_pool=(2,2), stride=(1,1), padding=(0,0), padding_pool=(0,0))

        self.linear = nn.Sequential(nn.Linear(in_features=3072, out_features=1024),
                                    nn.LeakyReLU(),
                                    nn.Dropout(0.1),
                                    nn.Linear(1024, 512),
                                    nn.LeakyReLU(),
                                    nn.Dropout(0.1),
                                    nn.Linear(512, 14))

    def forward(self, x):
        features = self.conv3(self.conv2(self.conv1(x)))
        features = features.view(features.shape[0], -1)
        return self.linear(features)

def initYOLOModel(modelpath, conf=0.25, iou=0.45):
    yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', modelpath)
    yolo_model.agnostic = True
    yolo_model.conf = conf
    yolo_model.iou = iou
    return yolo_model


def inferenceYOLO(model, imgpath, res):
    results = model(imgpath, size=res)
    return results


def initKeypoint(modelpath):
    model = BasicCNN()
    model.load_state_dict(torch.load(modelpath,map_location=torch.device('cpu')))
    return model


def cropResizeCones(yolo_results, image, margin):
    img_h = len(image)
    img_w = len(image[0])
    bounding_boxes = yolo_results.pandas().xyxy[0]
    cones_imgs = []
    classes = []
    cropped_img_corners = []
    for j in range(len(bounding_boxes)):
        bb_dict = bounding_boxes.iloc[j].to_dict()
        
        # Find corners of cropped images
        xmin = max(math.floor(bb_dict['xmin'])-margin, 0)
        xmax = min(math.floor(bb_dict['xmax'])+margin, img_w-1)
        ymin = max(math.floor(bb_dict['ymin'])-margin, 0)
        ymax = min(math.floor(bb_dict['ymax'])+margin, img_h-1)
        
        # Stack cropped images
        cone_img = image[ymin:ymax, xmin:xmax]
        cone_img = cv2.resize(cone_img,dsize=(48,64))
        cones_imgs.append(cone_img)
        
        # Stack corners of cropped images
        cropped_img_corners.append([xmin, ymin, xmax, ymax])

        # Stack classes of cropped images
        classes.append(bb_dict['class'])

    return cones_imgs, classes, cropped_img_corners


def runKeypoints(cones_imgs, keypoints_model):
    cones_imgs_list = []
    
    # Convert images to a Pytorch-friendly format
    for j in range(len(cones_imgs)):
        cones_imgs_list.append(torch.from_numpy(cones_imgs[j].transpose(2,0,1)).unsqueeze(0).float())
    cones_imgs_tensor = torch.cat(cones_imgs_list, dim=0)
    
    # Inference
    predictions = keypoints_model(cones_imgs_tensor).cpu().detach().numpy()
    predictions = predictions.reshape(predictions.shape[0], 7, 2)

    return predictions


def rt_converter(camera, pnp_dist):
    # Takes distance calculated by solvePnP and calculates range,theta from car CoG based on the camera used
    if camera == 'left':
        x = np.cos(math.pi*34/180)*np.cos(math.pi*9.5/180)*pnp_dist[2] + np.sin(math.pi*34/180)*pnp_dist[0] - np.sin(math.pi*9.5/180)*np.cos(math.pi*34/180)*pnp_dist[1] - 31
        y = -np.sin(math.pi*34/180)*np.cos(math.pi*9.5/180)*pnp_dist[2] + np.cos(math.pi*34/180)*pnp_dist[0] + np.sin(math.pi*34/180)*np.sin(math.pi*9.5/180)*pnp_dist[1] - 10
        r = np.sqrt(x**2 + y**2)
        theta = math.atan2(y, x)
    elif camera == 'right':
        x = np.cos(math.pi*34/180)*np.cos(math.pi*9.5/180)*pnp_dist[2] - np.sin(math.pi*34/180)*pnp_dist[0] - np.sin(math.pi*9.5/180)*np.cos(math.pi*34/180)*pnp_dist[1] - 31
        y = np.sin(math.pi*34/180)*np.cos(math.pi*9.5/180)*pnp_dist[2] + np.cos(math.pi*34/180)*pnp_dist[0] - np.sin(math.pi*34/180)*np.sin(math.pi*9.5/180)*pnp_dist[1] + 10
        r = np.sqrt(x**2 + y**2)
        theta = math.atan2(y, x)
    elif camera == 'center':
        x = np.cos(math.pi*9.5/180)*pnp_dist[2] - np.sin(math.pi*9.5/180)*pnp_dist[1] - 30
        y = pnp_dist[0]
        r = np.sqrt(x**2 + y**2)
        theta = math.atan2(y, x)
    return round(r[0]/100,3), round(theta,4)


def finalCoordinates(camera, classes, cropped_img_corners, predictions, OffsetY):
    rt = []
    for j in range(len(classes)):
        cone_keypoints = []
        
        # Outputs the coordinates of keypoints on the full image (not just ROI)
        for k in range(7):
            x = predictions[j][k][0]*(cropped_img_corners[j][2] - cropped_img_corners[j][0])/48 + cropped_img_corners[j][0]
            y = OffsetY + predictions[j][k][1]*(cropped_img_corners[j][3] - cropped_img_corners[j][1])/64 + cropped_img_corners[j][1]
            cone_keypoints.append([x,y]) 
        cone_keypoints_numpy = np.array(cone_keypoints)
        
        # Depending on lens used choose an intrinsic camera matrix
        if camera == 'center':
            cameraMatrix = np.array([[2500, 0, 640], [0, 2500, 512], [0, 0, 1]])
            distCoeffs = np.array([[0, 0, 0, 0, 0]])
        elif (camera=='left' or camera=='right'):
            cameraMatrix = np.array([[1250, 0, 640], [0, 1250, 512], [0, 0, 1]])
            distCoeffs = np.array([[0, 0, 0, 0, 0]])            
        
        # Depending on the cone class set real coordiantes of keypoints
        if classes[j] == 3:
            real_coords = np.array([[7.4, 2.7, 0],
                        [5.8, 11.9, 0],
                        [4.3, 20.5, 0],
                        [0, 32.5 ,0],
                        [-4.3, 20.5, 0],
                        [-5.8, 11.9, 0],
                        [-7.4, 2.7, 0]]) #WROOOONG
        elif classes[j] < 3:
            real_coords = np.array([[7.4, 2.7, 0],
                        [5.8, 11.9, 0],
                        [4.3, 20.5, 0],
                        [0, 32.5 ,0],
                        [-4.3, 20.5, 0],
                        [-5.8, 11.9, 0],
                        [-7.4, 2.7, 0]])
        
        # Use solvePnP to get cone position in camera frame and then find range,theta from car CoG
        _, _, translation_vector = cv2.solvePnP(real_coords, cone_keypoints_numpy, cameraMatrix, distCoeffs, cv2.SOLVEPNP_IPPE)
        rt.append(rt_converter(camera, translation_vector))

    return rt

def runPerception(img_path, camera, yolo_path, keypoints_path, OffsetY):
    
    # Initialize YOLO and Keypoints model
    yolo_model = initYOLOModel(yolo_path)
    keypoints_model = initKeypoint(keypoints_path)
    
    # Read image and convert to RGB
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Get YOLO results
    yolo_results = inferenceYOLO(yolo_model, img, 1280)

    # Get bounded images
    cones_imgs, classes, cropped_img_corners = cropResizeCones(yolo_results, img, 3)
    
    if len(cones_imgs) > 0:
        # Get Keypoints predictions
        keypoints_predictions = runKeypoints(cones_imgs, keypoints_model)
        # Get range,theta for each cone in car frame
        final_coordinates = finalCoordinates(camera, classes, cropped_img_corners, keypoints_predictions, OffsetY)
        for i in range(len(classes)):
            print("Class: ", classes[i], " Range: ", final_coordinates[i][0], " Theta: ", final_coordinates[i][1])
        return classes, final_coordinates
    else:
        print("No cones found")
        return

runPerception('sample_img.jpg', 'center', 'best.pt', 'KeypointsNet(333).pt', 0)